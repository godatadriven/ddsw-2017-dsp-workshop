{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the convolution networks module we have seen how we can use pre-trained neural networks, and use the features they learnt to solve new (supervised) learning tasks.\n",
    "\n",
    "\n",
    "This approach to machine learning is referred to as `Transfer learning`, that is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned. From Stanford's CS231 machine learning class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.**\n",
    "\n",
    "## Transfer learning with MNIST data\n",
    "\n",
    "We have seen during the lecture how to implement a CNN using the Keras framework, and how we can exploit a pre-trained network to extract features for image classification.\n",
    "\n",
    "In this lab we'll combine both approaches. You are asked to train a Convolutional Neural Network on the first five digits of the MNIST dataset, freeze and tune dense layers to classify the remaining 5. You are encouraged to review and incorporate the programming examples provided in the lecture material (see `convnet.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and split the MNIST data set into train and test **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: remember the `load_data()` method in `keras.datasets.mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Train a simple convnet on the MNIST dataset the first 5 digits [0..4]. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen how to train a CNN on MNIST data during the lecture. Let's reorganize the code a bit to make it more reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(model, train, test):\n",
    "    # using global variables for better reusability :joy:\n",
    "    global height, width, n_classes\n",
    "    \n",
    "    X_train, y_train = train\n",
    "    X_test, y_test = test\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    # we have to preprocess the data into the right form\n",
    "    X_train = X_train.reshape(n_train, 1, height, width).astype('float32') / 255\n",
    "    X_test = X_test.reshape(n_test, 1, height, width).astype('float32') / 255\n",
    "\n",
    "    y_train = to_categorical(y_train, n_classes)\n",
    "    y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          nb_epoch=3,\n",
    "          validation_data=(X_test, y_test))\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: it might be useful to store convolution and dense layers in separate tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_filters = 32\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "conv_layers =  (Convolution2D(n_filters, filter_size, filter_size, border_mode='valid', \n",
    "                              input_shape=(height, width, 1)),\n",
    "                Activation('relu'),\n",
    "                Convolution2D(n_filters, filter_size, filter_size),\n",
    "                Activation('relu'),\n",
    "                Flatten())\n",
    "dense_layers = (Dense(128), \n",
    "                Activation('relu'),\n",
    "                Dense(n_classes),\n",
    "                Activation('softmax'))\n",
    "\n",
    "for layer in conv_layers:\n",
    "    model.add(layer)\n",
    "    \n",
    "for layer in dense_layers:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train a model on the 0-5 digits. `y_train` contains the digit labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: we can select the target digits using `numpy`'s indexing capabilities. You can use `numpy`'s indexing functionality to select which classes to train on. You should select only datapoints labeled as < 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/1\n",
      "30596/30596 [==============================] - 45s - loss: 0.1164 - acc: 0.9673 - val_loss: 0.0273 - val_acc: 0.9914\n",
      "5139/5139 [==============================] - 2s     \n",
      "Test score: 0.0273331785328\n",
      "Test accuracy: 0.991438022962\n"
     ]
    }
   ],
   "source": [
    "train_cnn(model,\n",
    "          (X_train[y_train < 5], y_train[y_train < 5]),\n",
    "          (X_test[y_test < 5], y_test[y_test < 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 2. Freeze the convolution layers and fine-tune the dense layers for the classification of digits [5..9]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of this approach, to the results obtained by training and tuning an end-to-end network. Hint: review the `trainable` property of `Convolution2D` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in conv_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[5].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transfer step we train the model using `train_cnn()`, while `fit`ting (fine-tune) only the dense layers. You should select only data points labelled as >= 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/1\n",
      "29404/29404 [==============================] - 24s - loss: 0.3672 - acc: 0.9290 - val_loss: 0.0589 - val_acc: 0.9821\n",
      "4861/4861 [==============================] - 3s     \n",
      "Test score: 0.058881396084\n",
      "Test accuracy: 0.982102447946\n"
     ]
    }
   ],
   "source": [
    "train_cnn(model,\n",
    "            (X_train[y_train >= 5], y_train[y_train >= 5]),\n",
    "            (X_test[y_test >= 5]  , y_test[y_test >= 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time is almost cut in half and accuracy is comparable to performance of the model trained on all digits (see lecture notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened in the transfer step?\n",
    "    \n",
    "The convolution layers act as a feature extraction component. When fitting the model on the [5..9] numbers, \n",
    "after freezing the layers with `layer.trainable=False` we \"transter\" the features that the model learnt on [0..4] to the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " [1] [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)\n",
    "\n",
    " [2] [CNN Features off-the-shelf: an Astounding Baseline for Recognition](https://arxiv.org/abs/1403.6382)\n",
    " \n",
    " [3] [Stanford CS231, transfer learning](http://cs231n.github.io/transfer-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
