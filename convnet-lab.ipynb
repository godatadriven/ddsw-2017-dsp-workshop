{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the convolution networks module we have seen how we can use pre-trained neural networks, and use the features they learnt to solve new (supervised) learning tasks.\n",
    "\n",
    "\n",
    "This approach to machine learning is referred to as `Transfer learning`, that is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned. From Stanford's CS231 machine learning class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.**\n",
    "\n",
    "## Transfer learning with MNIST data\n",
    "\n",
    "We have seen during the lecture how to implement a CNN using the Keras framework, and how we can exploit a pre-trained network to extract features for image classification.\n",
    "\n",
    "In this lab we'll combine both approaches. You are asked to train a Convolutional Neural Network on the first five digits of the MNIST dataset, freeze and tune dense layers to classify the remaining 5. You are encouraged to review and incorporate the programming examples provided in the lecture material (see `convnet.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and split the MNIST data set into train and test **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: remember the `load_data()` method in `keras.datasets.mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your answer here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height, width = 28, 28\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Train a simple convnet on the MNIST dataset the first 5 digits [0..4]. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen how to train a CNN on MNIST data during the lecture. Let's reorganize the code a bit to make it more reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(model, train, test):\n",
    "    # using global variables for better reusability :joy:\n",
    "    global height, width, n_classes\n",
    "    <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: it might be useful to store convolution and dense layers in separate tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_filters = <your code here>\n",
    "filter_size = <your code here>\n",
    "pool_size = <your code here>\n",
    "\n",
    "conv_layers =  <your code here>\n",
    "dense_layers = <your code here>\n",
    "\n",
    "for layer in conv_layers:\n",
    "    model.add(layer)\n",
    "    \n",
    "for layer in dense_layers:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to train a model on the 0-5 digits. `y_train` contains the digit labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: we can select the target digits using `numpy`'s indexing capabilities. You can use `numpy`'s indexing functionality to select which classes to train on. You should select only datapoints labeled as < 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 2. Freeze the convolution layers and fine-tune the dense layers for the classification of digits [5..9]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of this approach, to the results obtained by training and tuning an end-to-end network. Hint: review the `trainable` property of `Convolution2D` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transfer step we train the model using `train_cnn()`, while `fit`ting (fine-tune) only the dense layers. You should select only data points labelled as >= 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance of this model? What about training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened in the transfer step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " [1] [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)\n",
    "\n",
    " [2] [CNN Features off-the-shelf: an Astounding Baseline for Recognition](https://arxiv.org/abs/1403.6382)\n",
    " \n",
    " [3] [Stanford CS231, transfer learning](http://cs231n.github.io/transfer-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
